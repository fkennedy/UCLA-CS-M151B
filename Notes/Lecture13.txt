Week 8 - Wednesday
Chapter 5 - Large and Fast: Exploiting Memory Hierarchy

5_1
Memory Technology
    Large array of different types of memory for computing
    Static RAM (SRAM)
        Typically used in caches
        Most expensive in price ber byte
        Lowest latency
        0.5ns - 2.5ns, $2000-$5000 per GB
    Dynamic RAM
        Main memory
        Higher latency than SRAM but significantly cheaper
        50ns - 70ns, $20-$75 per GB
    Magnetic disk
        Hard drives on an actual device
        Less cost but extreme latency
        5ms - 20ms, $0.20-$2 per GB
    Difficult to get ideal memory that have rapid access time
        Access time of SRAM
        Capacity and cost/GB of disk
        There has to be hiearchy to overcome this problem

DRAM Technology
    Implemented as a single capacitor per bit
        When read data out of capacitor, have to charge back into it upon read
        Refreshing adds to the latency
    Volatile - lose power lose data
    Higher capacity than cache but not as high as hard disk
    Between cache and hard disk for latency

Flash Storage
    Nonvolatile, semiconductor storage
        Doesn't lose data when it's turned off
    Faster to access than hard disk
    A little bit more expensive
        Between disk and DRAM
            Can't get same capacity like disk
    Used as conventional main memory, lacks latency

Disk Storage
    Nonvolatile, rotating magnetic storage
        All the rotation contributes to the latency

Principle of Locality
    How to build around these limitations?
    Porgrams access a small proportion of their address space at any time
    Locality aspect means that accessing of data over and over again
    Temporal locality - exactly the same data
        Items accessed recently are likely to be accessed again soon
            Instructions in a loop
            Induction variables
    Spatial locality - things that are close together in time
        Items near those accessed recently are likely to be accessed soon
            Sequential instruction access
            Array data

Taking Advantage of Locality
    Form a memory hierarchy
        Highest point is cache memory
            First place for memory access
            Small but high turnaround speed (low latency)
        Next layer is larger caches (L2/L3)
            A little bit larger capacity but turnaround speed will not be as fast as L1
        Main memory
            Large penalty for access time
            Greater capacity means harder to find
        Disk
            Everything that we want is here
            Highest cost in terms of latency
    If we access something in DRAM or in disk, we'll bring to a higher level in the hierarchy like cache
        Things that are close to the data
        Things that are recently accessed
            We can exploit locality in the address strip

Memory Hierarchy Levels
    Block (aka line): unit of copying
        May be multiple words
    If accessed data is present in upper level
        Hit: access satisfied by upper level
            Hit ratio: hits/accesses
    If absent
        Miss: block copied from lower level
            Time taken: miss penalty
            Miss ratio: misses/accesses
                      = 1 - hit ratio
        Then accessed data supplied from upper level

5_2
Cache Memory
    Cache memory
        The level of the memory hierarchy closest to the CPU
    Questions:
        How do we organize the cache memory in the architectural sense to the best extent
        How do we manage data that goes in and out?
    Multiple levels of caches
        L1-L3

Direct Mapped Cache
    Lowest overhead cache to implement
    Cache can't hold everything
        8 entry cache can only hold 8 blocks of instructions
        Location of where I looked will be determined by the address
    Only one choice
        Block address modulo number of blocks in cache
            Number of blocks is a power of 2
        Use low-order address bits

Tags and Valid Bits
    How do we know which particular block is stored in a cache location?
        Store block address as well as the data
        We only need the high-order bits
        This is called tag
    What if there was no data in a location?
        This is called valid
            1 - present
            0 - not present
        Initially 01

Cache Example
    Table example:
        Index   V   Tag Data
        000     N   
        000     N   
        000     N   
        000     N   
        000     N   
        000     N   
        000     N   
        000     N   
    Instructions:
        Word addr   Binary addr Hit/Miss    Cache block

    Instruction:
        22          10 110      Miss        110
    First time access will be a miss
        Initially not in the cache
        So we bring it to the cache
            Upper 2 bits are used for the tag
            Lower 3 bits are used for the index in the cache
            Data will be Mem[10110]
    Table will look like:
        Index   V   Tag Data
        000     N   
        001     N   
        010     N   
        011     N   
        100     N   
        101     N   
        110     Y   10  Mem[10110]   
        111     N   

    Instruction:
        26          11 010      Miss        010
    First time access will be a miss
        11 will be tag, 010 will be Index
        Data will contain contents of Mem[11010]
    Table will look like:
        Index   V   Tag Data
        000     N   
        001     N   
        010     Y   11  Mem[11010]   
        011     N   
        100     N   
        101     N   
        110     Y   10  Mem[10110]   
        111     N

    Instructions:
        22          10 110      Hit         110
        26          11 010      Hit         010
    Second time access will be a hit
        Won't go into memory
        Instruction 22:
            Check index 110
            Check tag 10
            If the same then it's a hit!
            Temporal locality

    Instructions:
        16          10 000      Miss        000
        3           00 011      Miss        011
        16          10 000      Hit         000
    Table will look like:
        Index   V   Tag Data
        000     Y   10  Mem[10000]   
        001     N   
        010     Y   11  Mem[11010]   
        011     Y   00  Mem[00011]   
        100     N   
        101     N   
        110     Y   10  Mem[10110]   
        111     N
    First time access will be a miss
    Second time access will be a hit

    Instructions:
        18          10 010      Miss        010
    Table will look like:
        Index   V   Tag Data
        000     Y   10  Mem[10000]   
        001     N   
        010     Y   10  Mem[10010]   
        011     Y   00  Mem[00011]   
        100     N   
        101     N   
        110     Y   10  Mem[10110]   
        111     N
    We have to kick out the previous instrution to bring in new instruction
        Instruction 18 will replace instruction 22 in the cache table
        Even though there are empty cache blocks, we have to replace because it is directly-mapped
            Too many things going into the same hash element

5_3
Example: Larger Block Size
    64 blocks, 16 bytes/block
        To what block number does address 1200 map to?
            Block address = ⌊1200/16⌋ = 75
            Block number = 75 modulo 64 = 11

    Tag - 22 bits
    Index - 6 bits
    Offset - 4 bits

Block Size Considerations
    Larger blocks should reduce miss rate
        Due to spatial locality
    But in a fixed-sized cache in smaller size
        Larger blocks => fewer total block caches
            More competition => increased miss rate
        Larger blocks => pollution
    Larger miss penalty
        Can override benefit of reduced miss rate
        Early restart and critical-word-first can help
            Take the part of the block that is going to be used by the instruction causing a miss
            Send that first
        Must consider larger blocks will take longer time to put back into memory

Cache Misses
    On cache hit, CPU proceeds normally
    On cache miss
        Data cache miss
            Complete data access
            Stall the CPU pipeline
                Until miss is satisfied
            Reason is because:
                Fetching from block from next level of hierarchy
        Instruction cache miss
            Stall front of pipeline until miss is satisfied
            Restart instruction fetch
        
Write-Through
    Loads and stores are going to be able to cause misses in the cache
    How do you handle stores and how do you handle the disparity that exists between the cached data and data that is actually in the memory?
    For example
        If I had a store that is going to change an address x in the cache
            It writes some data to address x in the cache
                Inconsistent to memory
            Modify the contents of the lower levels of hierarchy?
            Or accumulate as much modification as possible, when evicted, write to memory
    If writing to cache, also writing to main memory
        Use bandwidth of memory hierarchy to keep writing multiple times
    Benefit:
        Block evicted, don't have to wait for update
            Update ahead of time to reduce penalty

Write-Back
    Alternative to Write-Through
    If writing to cache, update dirty bit
        If a block is dirty, it essentially means that the block was recently updated (different from the memory)
            This only happens if we make use of locality - accessing memory over and over again
    When a dirty block is replaced
        Write it back to memory
        Can use a write buffer to allow replacing block to be read first

Write Allocaiton
    What happens on a write miss?
        Have store to write data in cache, but it's not in cache
    Write allocate
        When we have a write miss, we allocate a block in the cache for that miss
            Bring data in and modify it
    Write-through able to use both
        Write around or write to the same level
    Write-back
        Fetch the block
            Can't change the dirty bit if in memory

Measuring Cache Performance
    Components of CPU time
        Program execution cycles
            Includes cache hit time
        Memory stall cycles
            Mainly from cache misses (penalties)
    With simplifying assumptions:
        Memory stall cycles = Memory access/Program * Miss rate * Miss penalty
                            = Instructions/Program * Miss/Instruction * Miss penalty
Cache Performance Example
    Given
        I-cache miss rate = 2%
        D-cache miss rate = 4%
        Miss penalty = 100 cycles
        Base CPI (BCPI) (ideal cache) = 2
        Load & stores are 36% of instructions
    Miss cycles per instruction
        I-cache: 0.02 * 100 = 2
        D-cache: 0.36 * 0.04 * 100 = 1.44
        Actual CPI = 2+2+1.44 = 5.44
        Ideal CPU is 5.44/2 = 2.72 times faster

Average Access Time
    Hit time is also important for performance
    AMAT - Average memory access time
        AMAT = Hit time + Miss rate * Miss penalty
    Example:
        CPU with 1ns clock
        Hit time = 1 cycle
        Miss penalty = 20 cycles
        I-cache miss rate = 5%
        AMAT = 1+0.05 * 20 = 20ns
            2ns/1ns = 2 cycles per instruction

Performance Summary
    When CPU performance is increased
        Miss penalty becomes more significant
    Decreasing BCPI
        Greater proportion of time is spent on memory stalls
    Increasing clock rate
        Memory stalls account for more CPU cycles
    Can't neglect cache behavior when evaluating system performance

5_4
Associative Caches
    Associativity - idea of how many different entries in the cache may we use for a particular block in memory
    Direct map - one end of the associative spectrum
        Con: No flexibility
            More flexibility means that we have to look at more than one location
        Pro: Only look at one location
    Fully associative
        Complete flexibility on block placement
        Have to search everywhere for a block
        Comparator per entry (expensive in terms of power)
    N-way set associative
        Compromise between fully associative and direct map
        Indicates how many entries we can associate a particular address with
        Organize the cache into sets
            Each set would have n entries
                Particular address will be able to go to these n entries
        Need n comparators
            Limit the number of blocks in the cache that you have to look at
                Index will determine which set to look

Associative Example
    8 blocks in the cache
    Direct mapped - directly go to the block number
    Set associative - look at set 0 (there are 4 sets)
    Fully associatie - go through all the blocks

Spectrum of Associativity
    For a cache with 8 entries
        1-way: Direct mapped
        2-way: 4 sets of 2 entries each
        4-way: 2 sets of 4 entries each
        8-way: Fully associative
    With greater associativity, you get greater flexibility
    With smaller associativity, you get smaller flexibility
        Higher cache miss rate

Associativity example
    Compare 4-block caches
    3 ways to do this:
        Direct mapped
        2-way set associative
        Full associative
    Block access sequence: 0, 8, 0, 6, 8

    Direct mapped
        Miss
        Miss
        Miss
        Miss
        Miss
    Mem[8] and Mem[0] can't coexist because they have the same cache index
    Mem[6] and Mem[8] can coexist together because they have different indexes, however, there won't be any hits because we're no longer accessing Mem[6] or Mem[8]

    2-way set associative
        Miss
        Miss
        Hit
        Miss
        Miss
    Second access of Mem[0] is a hit because it's still in the set
    We replace Mem[8] with Mem[6] because Mem[0] was a recent hit so we keep that
    We replace Mem[0] with Mem[8] because Mem[6] was recently inputted into the table 

    Fully associative
        Miss
        Miss
        Hit
        Miss
        Hit
    Second access of Mem[0] is a hit because it's in the cache content
    Second access of Mem[8] is a hit because it's in the cache content
    We keep adding the Mems into the cache content  because we have 4 spaces

How Much Associativity
    Increased associativity decreases miss rate
        But with diminishing returns
    Simulation of a system with 64KB D-cache, 16-word blocks, SPEC2000
        1-way: 10.3% miss rate
        2-way: 8.6% miss rate
        4-way: 8.3% miss rate
        8-way: 8.1% miss rate

Replacement Policy in cache
    Direct mapped: no choice
    Set associative
        Prefer non-valid entry if there is one
        Otherwise, choose among entries in the set
        Policies:
                Least-recently used (LRU)
                    Choose the one unused for the longest time
                        Simple for 2-way
                            Use single bit to determine LRU
                        Manageable for 4-way
                            Use 2 bits to determine LRU
                        Too hard for anything beyond
                Random
                    Will give approximately the same performance as LRU for high associativity
                FIFO/LIFO